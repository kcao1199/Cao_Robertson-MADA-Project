---
title: "Data Analysis and Modeling of Teen Vaccination Surveys"
date: '`r format(Sys.Date())`'
Author: Kelly Cao and Rachel Robertson
output: html_document
editor: 
  markdown: 
    wrap: sentence
---
# LASSO using Tidymodels
```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```
We will start by opening the libraries that we will need for creating a LASSO regression model for the data
```{r}
library(tidymodels) # use tidymodels framework
library(ggplot2) # producing visual displays of data
library(dplyr) # manipulating and cleaning data
library(here) # making relative pathways
library(glmnet) # for LASSO regression engine
library(doParallel) # for parallel processing
library(rsample) # for cross validation
library(yardstick) # for metrics
library(mlbench)
library(vip)
library(gt)
```

Now, we will load the preprocessed data.
```{r}
# Load and preprocess data
data_location <- here::here("data","processed-data","cleandata1.rds")
mydata <- readRDS(data_location)

# Remove rows with missing values
mydata$SEX <- droplevels(mydata$SEX, exclude = c("DON'T KNOW", "MISSING IN ERROR", "REFUSED"))
mydata$INS_STAT2_I <- droplevels(mydata$INS_STAT2_I, exclude = "MISSING Data")
mydata$STATE <- droplevels(mydata$STATE, exclude = "Missing Data")
mydata$MOBIL_1 <- droplevels(mydata$MOBIL_1, exclude = c("DON'T KNOW", "MISSING IN ERROR", "REFUSED"))
mydata$FACILTY <- droplevels(mydata$FACILITY, exclude = "Missing Data")
mydata$P_UTDHPV <- droplevels(mydata$P_UTDHPV, exclude = "Missing Data")
```
Next, we will split the data into training and testing groups so that the LASSO regression has a comparison group.
```{r}
# Split data into training and testing datasets
set.seed(123) # seed for reproducibility
split_data <- initial_split(mydata, prop = 0.8) # 80% split for training/testing data
train_data <- training(split_data)
test_data <- testing(split_data)
```
Next, we will specify the LASSO recipe, or the regression formula. This will contain the predictors of interest used for previous analysis and `P_UTDHPV` as the outcome. 
```{r}
# Create recipe containing predictors of interest
lasso_rec <- recipe(P_UTDHPV ~ AGE + SEX + STATE + INS_STAT2_I + INCQ298A + INS_BREAK_I + INCPOV1 + RACEETHK + EDUC1 + LANGUAGE + MOBIL_1 + RENT_OWN + FACILITY, data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes())  %>% # convert categorical variables (nominal) into dummy variables
  step_rm(all_outcomes(), -all_outcomes()) #remove the outcome variables from the dataset

# Prepare the recipe
lasso_prep <- lasso_rec %>%
  prep()

```
Now I will implement the function. This involves setting the specific LASSO regression engine, creating a workflow, and then extracting the fit of the model.
```{r}
# Specify logistic regression model
lasso_spec <- logistic_reg(penalty = 0.05, mixture = 1) %>%
  set_engine("glmnet")

# Create workflow
lasso_wf <- workflow() %>%
  add_recipe(lasso_rec)

# Fit the model using training data
lasso_fit <- lasso_wf %>%
  add_model(lasso_spec) %>%
  fit(data = train_data)

# Extract and tidy model parameters
lasso_fit %>%
  extract_fit_parsnip() %>%
  tidy()

# Extract and tidy model parameters
lasso_coef <- lasso_fit %>%
  extract_fit_parsnip() %>%
  tidy()

# Filter out predictors with non-zero coefficients
lasso_nonzero_coef <- lasso_coef %>%
  filter(estimate != 0)

# View the remaining predictors with non-zero coefficients
num_observations <- nrow(lasso_nonzero_coef)
print(num_observations)

```
  
It seems like the LASSO model is penalizing the coefficients too heavily, resulting in most of them being shrunk to zero. We are going to try lowering the penalty parameter (penalty) to see if it is effective in solving our problem.

```{r}
# Specify logistic regression model
lasso_spec2 <- logistic_reg(penalty = 0.005, mixture = 1) %>%
  set_engine("glmnet")

# Fit the model using training data
lasso_fit2 <- lasso_wf %>%
  add_model(lasso_spec2) %>%
  fit(data = train_data)

# Extract and tidy model parameters
lasso_fit2 %>%
  extract_fit_parsnip() %>%
  tidy()

# Extract and tidy model parameters
lasso_coef <- lasso_fit2 %>%
  extract_fit_parsnip() %>%
  tidy()

# Filter out predictors with non-zero coefficients
lasso_nonzero_coef <- lasso_coef %>%
  filter(estimate != 0)

# View the remaining predictors with non-zero coefficients
num_observations <- nrow(lasso_nonzero_coef)
print(num_observations)

```

```{r}
# Make predictions for the first LASSO model
predictions <- lasso_fit %>%
  augment(new_data = train_data)

# Calculate ROC AUC
roc_auc <- predictions %>%
  roc_auc(truth = P_UTDHPV, .pred_UTD)
print(roc_auc)

# Make predictions for the LASSO model with lower penalty
predictions2 <- lasso_fit2 %>%
  augment(new_data = train_data)

# Calculate ROC AUC
roc_auc <- predictions2 %>%
  roc_auc(truth = P_UTDHPV, .pred_UTD)
print(roc_auc)

```
When attempting to determine how lowering the penalty effected the ability of the model, we produced some predictions for the model with a higher penalty and the model with a very low penalty. Lowering the penalty from 0.05 to 0.005 improved the ROC AUC to from 0.5 to 0.37. This suggests that the second model is more informative. However, to improve the model further, we should find the ideal penalty parameter.

To determine the ideal penalty, we are going to tune the model. 

# Tuning LASSO model
Any penalty above 0.05 resulted in all of the predictor coefficient reduced to zero. When performing the following tuning, I set the penalty grid from 0.001 to 0.05. 
```{r}
# Define a grid of penalty values to tune over
penalty_grid <- expand.grid(penalty = seq(0.001, 0.05, length.out = 20))

# Set up parallel processing for faster tuning
registerDoParallel()

set.seed(123) # ste seed for reproducability

# LASSO model specification
lasso_model_tuned <- 
  logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification")  # Ensure correct mode for classification problem

# LASSO Workflow
lasso_workflow_tuned <- 
  workflow() %>%
  add_model(lasso_model_tuned) %>%
  add_recipe(lasso_rec)

# Set up the grid for tuning
lasso_tuned <- 
  lasso_workflow_tuned %>%
  tune_grid(
    resamples = vfold_cv(train_data, v = 5, repeats = 5), 
    grid = penalty_grid,
    control = control_grid(verbose = TRUE)
  )

# View tuning results
tuned_plot <- autoplot(lasso_tuned)
tuned_plot + ggtitle( "Tuned LASSO Model")

# Collect tuning results
tuning_results <- lasso_tuned %>%
  collect_metrics()

# Filter for ROC AUC metric
roc_auc <- tuning_results %>%
  filter(.metric == "roc_auc")

# View the ROC AUC values
print(roc_auc)

# find the best penalty for ROC AUC
best_roc <- lasso_tuned %>%
  select_best(metric = "roc_auc")
best_roc

```

When viewing the metrics of the tuning, we can see that the regularization is directly correlated with the reduction in roc_auc and accuracy and the increase in brier_Class. The accuracy is later shown to start increasing suggesting underfitting starts occurring around 0.01. Just reviewing the roc_auc alone, the best performance occurs when the penalty is at its lowest.

# LASSO Re-fit
Now, we will run the LASSO regression again, but with the tuned penalty parameter of 0.001. 
```{r}
lasso_tune_spec <- logistic_reg(penalty = 0.001, mixture = 1) %>%
  set_engine("glmnet")

# Create workflow
lasso_tune_wf <- workflow() %>%
  add_recipe(lasso_rec)%>%
  add_model(lasso_tune_spec)


# Fit the model using training data
lasso_tune_fit <- lasso_tune_wf %>%
  fit(data = train_data)

# Extract and tidy model parameters
lasso_tune_coef <- lasso_tune_fit %>%
  extract_fit_parsnip() %>%
  tidy()

# Filter out predictors with non-zero coefficients
lasso_nonzero_coef <- lasso_tune_coef %>%
  filter(estimate != 0)

# View the remaining predictors with non-zero coefficients
num_observations <- nrow(lasso_nonzero_coef)
print(num_observations)

# Make predictions for ROC_AUC with the tuned model
predictions3 <- lasso_tune_fit %>%
  augment(new_data = train_data)

# Calculate ROC AUC
roc_auc <- predictions3 %>%
  roc_auc(truth = P_UTDHPV, .pred_UTD)
print(roc_auc)
```
The ROC_AUC for this model with a penalty parameter of 0.01 is now 0.39. This is slightly worse than the model with a penalty of 0.05. However, the model with a lower penalty is more "unstable", so doing cross validation for the ROC_AUC may reveal which model is truly ideal. 

# Comparing Predictions
Now we will compare the two lasso models with the penalty parameters of 0.005 and 0.001 to determine which model performs best when presented with "new data". The new data is that split from the original data (the test data). 
```{r}
pen1_aug <- augment(lasso_fit2, test_data)
pen2_aug <- augment(lasso_tune_fit, test_data)
# Make find performance metrics with the tuned model
metrics <- metric_set(accuracy, f_meas) # create a set of metrics to test for classification
# Model tuned to parameter penalty = 0.005
penaltytune1 <- metrics(truth = P_UTDHPV, estimate = .pred_class, data = pen1_aug)
print(penaltytune1)
# Model tuned to parameter penalty = 0.01
penaltytune2 <- metrics(truth = P_UTDHPV, estimate = .pred_class, data = pen2_aug)
print(penaltytune2)
```
The performance metrics reveal a similar accuracy between both models, but a higher, thus better, F1 measure, from the LASSO model with the penalty of 0.001. 
```{r}
# Compare confusion matrices
# penalty 0.005
conf_mat(
  data = pen1_aug,
  truth = P_UTDHPV,
  estimate = .pred_class,
  dnn = c("Prediction", "Truth"),
  case_weights = NULL)
# penalty 0.001
conf_mat(
  data = pen2_aug,
  truth = P_UTDHPV,
  estimate = .pred_class,
  dnn = c("Prediction", "Truth"),
  case_weights = NULL)
```
The confusion matrix for neither of these models is ideal, as they both have a fairly high number of false positives, leading to a poor sensitivity. Because the model with a lower penalty parameter seems to perform better when presented with new data, I will look at the coefficients chosen by this model and determine which variables it deems most important to predicting HPV vaccine completion.
```{r}
# Finding importance of variables chosen
lasso_vars <- lasso_tune_fit %>%
  extract_fit_parsnip() %>% # use parsnip to extract to fit
  vip::vip() #use vip for visualizing the most important variables

# Modify the plot to set the color of the bars
lasso_vars_plot <- lasso_vars+
  aes(fill = "skyblue")+
  labs(title = "Top Predictors")

# Create a data frame with original and translated variable names
variable_translations <- data.frame(
  Original_Variable = rev(c("STATE_RHODE.ISLAND", "FACILITY", 
                        "STATE_VERMONT", "STATE_MISSISSIPPI", "STATE_NORTH.DAKOTA", "STATE_IOWA", 
                        "STATE_HAWAII", "RENT_OWN_DON.T.KNOW", "STATE_NEBRASKA", 
                        "INS_BREAK_I_CURRENTLY.UNINSURED.AND.NEVER.INSURED.SINCE.AGE.11")),
  Translated_Variable = rev(c("Rhode Island", "All Std School Teen Clinics or Other Facilities", 
                          "Vermont", "Mississippi", "North Dakota", "Iowa", "Hawaii", 
                          "Rent/Own Status (Don't Know)", "Nebraska", 
                          "Uninsured"))
)

# Add translations to the plot
lasso_vars_plot_with_translations <- lasso_vars_plot +
  scale_fill_manual(values = c("skyblue")) +  # Set fill color manually
  scale_x_discrete(labels = variable_translations$Translated_Variable) +  # Translate x-axis labels
  labs(title = "Top Predictors",
                x = "Variable")+  # Add title and update x-axis label
  scale_y_continuous(labels = function(x) strwrap(x, width = 10))

# Display the plot with translated variable names
print(lasso_vars_plot_with_translations)

# Save the plot as a PNG file
path = here("results", "figures", "top_lasso_predictors.png")
ggsave(filename = path, plot=lasso_vars_plot_with_translations) 
```
It seems that the most important factors for HPV vaccine completion are the state, facility and housing status. The states that have the highest predictive power of HPV vaccine completion are Rhode Island, Vermont, Mississippi, and North Dakota.
```{r}
# Extract and tidy model parameters
lasso_tune_coef <- lasso_tune_fit %>%
  extract_fit_parsnip() %>%
  tidy()

# Filter out predictors with non-zero coefficients
lasso_tune_coef <- lasso_tune_coef %>%
  filter(estimate != 0)
lasso_tune_coef
```


Note to Kelly: Perhaps we should try tuning the penalty to a different metric. May we can try tuning to F1 and see if that produces a different penalty value?
Note to self/ Discussion idea: Maybe vaccine policy or vaccine requirements to start school differs by state, so this might affect completion rates.